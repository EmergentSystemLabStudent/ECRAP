<!DOCTYPE html>
<html>
<head lang="en">
  <meta name="keywords" content="ECRAP">
  <title>ECRAP: Exophora Resolution and Classifying User Commands for Robot Action Planning by Large Language Models</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    .container {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 70px;
      flex-direction: column;
      text-align: center;
    }

    .fixed-width {
      width: 700px;
      margin: 10px auto;
    }

    .center-align {
      text-align: center;
    }

    .text-justify {
      text-align: justify;
    }

    video {
      width: 700px;
      height: auto;
      display: block;
      margin: 10px auto;
    }

    .list-centered {
      list-style: none; /* Remove default list style */
      padding: 0;
      text-align: left;
      margin: 0 auto; /* Center align the list */
      counter-reset: list-counter; /* Initialize the counter */
    }
    
    .list-centered li {
      margin-left: 1.5em; /* Indent list items */
      position: relative;
    }
    
    .list-centered li:before {
      content: counter(list-counter) ". "; /* Add counter before list items */
      counter-increment: list-counter; /* Increment the counter */
      position: absolute;
      left: -1.5em; /* Adjust position of numbers */
    }


    .col-md-8 {
      width: 700px;
      margin: 10px auto;
    }

    .github-link {
      display: flex;
      justify-content: center;
    }

    .github-link a {
      margin: 10px;
    }

    .image-container {
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
    }

    .image-container img {
      width: 700px;
      height: auto;
    }

    .image-caption {
      text-align: center;
      margin-top: 10px; /* Adjust the space between the image and the caption */
      font-style: italic;
      color: #555; /* Optional: change the caption color */
    }

    .video-container {
        margin: 10px;
        display: flex;
        justify-content: center;
    }
    
    .video-container iframe {
        width: 700px;
        height: 394px;
    }
  </style>
</head>
<body>
  <div class="container" id="main">
    <div class="row">
      <h2 class="col-md-12">
         ECRAP: Exophora Resolution and Classifying User Commands for Robot Action Planning by Large Language Models
      </h2>
    </div>
  </div>
  
  <p class="center-align">Akira Oyama*, <a href="https://scholar.google.com/citations?user=KPxSCJUAAAAJ&hl" target=“_blank” rel=“noopener noreferrer”>Shoichi Hasegawa</a>, <a href="https://scholar.google.com/citations?user=Y4qjYvMAAAAJ&hl" target=“_blank” rel=“noopener noreferrer”>Yoshinobu Hagiwara</a>, <a href="https://scholar.google.com/citations?user=jtB7J0AAAAAJ&hl" target=“_blank” rel=“noopener noreferrer”>Akira Taniguchi</a>, <a href="https://scholar.google.com/citations?user=dPOCLQEAAAAJ&hl" target=“_blank” rel=“noopener noreferrer”>Tadahiro Taniguchi</a></p>
  
  <div class="github-link">
    <a href="https://github.com/EmergentSystemLabStudent/ECRAP/tree/master" target=“_blank” rel=“noopener noreferrer”>Github</a>
    <!-- <a href="" target=“_blank” rel=“noopener noreferrer”>Paper</a> -->
    <a href="" target=“_blank” rel=“noopener noreferrer”>Slide</a>
  </div>


  <!-- <div class="video-container">
      <iframe src="https://www.youtube.com/embed/n8se-MgPi50?si=B5B60-8XXMO-j2CC" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
  </div> -->

  <div class="image-container">
    <img src="./abstract_v3.svg" alt="Overview of abstract">
    <div class="image-caption">Figure 1: Overview of our abstract</div>
  </div>
  
  <h2 class="fixed-width">Abstract</h2>
  <p class="center-align fixed-width text-justify">
    The ability to understand a variety of verbal instructions and perform tasks is important for daily life support robots.
    People's speech to the robot may include greetings and demonstratives, and the robot needs to act appropriately in such cases.
    In the case of instruction including demonstratives such as ``Bring me that cup,'' exophora resolution (ER) is needed to find the specific object corresponding to ``that cup.''
    Recently, action planning based on large language models (LLMs) has been necessary for robots to act and has been rapidly studied.
    However, in previous action planning research, it was difficult to identify the target object from language instructions including demonstratives, and to perform the planning desired by the user.
    This study aims to plan actions expected by the user from various user language queries, such as greetings or those containing demonstratives.
    We propose a method for action planning from a variety of language instructions by combining a task classification module, an ER framework, and LLMs.
    In the experiment, various queries were given, including queries containing demonstratives and queries that do not require the robot to move but only to respond, and the planning accuracy was compared to a baseline without an exophora resolution framework.
    The results show that the proposed method is approximately 1.3 times more accurate than the baseline.
  </p>

  <h2 class="fixed-width">Approach</h2>
  <p class="center-align fixed-width text-justify">
  </p>

  
  
  <div class="col-md-8">
  <h2 class="text-center">Citation</h2>
  <p class="text-justify">
    This paper was accepted on IEEE International Conference on Robotic Computing (IRC 2024), 
    and the BibTeX for the paper is below.
    <textarea id="bibtex" class="form-control" readonly rows="6" cols="105">
      @inproceedings{ecrap2024,
        title={ECRAP: Exophora Resolution and Classifying User Commands for Robot Action Planning by Large Language Models},
        author={Akira Oyama and Shoichi Hasegawa and Yoshinobu Hagiwara and Akira Taniguchi and Tadahiro Taniguchi},
        booktitle={IEEE International Conference on Robotic Computing (IRC)},
        year={2024, accepted}
}</textarea>
  </p>
</div>

  <div class="col-md-8">
    <h2 class="text-center">Other links</h2>
    <p class="text-justify">
      <ul>
        <li><a href="http://www.em.ci.ritsumei.ac.jp/" target=“_blank” rel=“noopener noreferrer”>Laboratory website</a></li>
        <li><a href="" target=“_blank” rel=“noopener noreferrer”>Demo video of this research</a></li>
        <li><a href="https://www.youtube.com/watch?v=UBgZGRG00eA" target=“_blank” rel=“noopener noreferrer”>Demo video of related research</a></li>
      </ul>
    </p>
  </div>
    
  <div class="col-md-8">
    <h2 class="text-center">Acknowledgements</h2>
    <p class="text-justify">
      This work was supported by JSPS KAKENHI Grants-in-Aid for Scientific Research (Grant Numbers JP23K16975, 22K12212), JST Moonshot Research \& Development Program (Grant Number JPMJMS2011).
    </p>
  </div>
  
  <script src="script.js"></script>
</body>
</html>
